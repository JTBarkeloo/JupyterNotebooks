{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nationwide Application Assessment for Computational Telematics\n",
    "# Jason Barkeloo\n",
    "\n",
    "Now let's import necessary libraries. <br>\n",
    "numpy for various linear algebra libraries<br>\n",
    "pandas for convenient file reading as well as dataframe structures that are convenient to work with<br>\n",
    "matplotlib for various basic plots<br>\n",
    "geopy for geodesic distances used for calculations in part 1\n",
    "sklearn, keras (using locally installed tensorflow) for neural network applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "import sys\n",
    "import sklearn\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: GPS Data\n",
    "## (1) What steps did you take to clean the data?\n",
    "To clean the data let's start by looking at it to see any abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips = pd.read_csv(\"C:/Users/JTBar/Documents/Telematics Exercise Files/sample_trips.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset 'sample_trips.csv' contains 9687 rows of data and 4 columns containing the information on 16 different trips (trip_nb), the local date time (local_dtm), latitude and longitudinal coordinates.  I'm reading it into a pandas dataframe for ease of use.\n",
    "\n",
    "For the most part the change from one row of local_dtm to the next suggest a rate of 1Hz data collection, however ##many times## this data has a longer sampling rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips.latitude.plot()\n",
    "sample_trips.longitude.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips[410:416].latitude, sample_trips[410:416].longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips[5660:5665].latitude, sample_trips[5660:5665].longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips[8480:8485].latitude, sample_trips[8480:8485].longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "Here the latitude and longitude cleaning is done.  The latitude cleaning is done first and should pick up all of the misplaced points as both gps coordinates are erroneous in these cases, the longitude is done still for completeness.  The latitude and longitude are required to be within 2 degrees of the median.  This allows the individual trips to be in an area a little over the size of the state of ohio.  I am assuming that the purpose of the data is to check day-to-day driving habits and not particularly long road trips.  This is a simplistic method based on looking at the data but achieves the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered = sample_trips[(np.abs(sample_trips['latitude']-sample_trips.latitude.median())<2.)]\n",
    "sample_trips_filtered = sample_trips_filtered[(np.abs(sample_trips_filtered['longitude']-sample_trips_filtered.longitude.median())<2.)]\n",
    "sample_trips_filtered = sample_trips_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen now that the latitude and longitude plots do not have any nonsensical jumps, let's dig a little deeper into these trips to see if anything else needs to be cleaned up (i.e. gps drift while at rest or large gps jumps that are inconsistent with the current rate of travel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(15,5))\n",
    "sample_trips_filtered.latitude.plot(ax=axs[0],title=\"Latitude\")\n",
    "sample_trips_filtered.longitude.plot(ax=axs[1],title=\"Longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting to look at the timesteps, first lets change them to a datetime stamp thats a little more intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered['local_dtm']=pd.to_datetime(sample_trips_filtered['local_dtm'],format='%d%b%y:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in some information to the dataframe based on the latitudude, longitude, timestamps to do analysis on more physically meaningful variables \n",
    "The variables being added are the change in position ('DeltaPos (mi)') and time ('DeltaTime (hr)') between two datapoints.  The Speed in miles per hour ('Speed MPH') and acceleration in miles per hour per second ('Accel MPHPS') are also calculated using very basic physics relationships (i.e. speed is change in position per time, instantaneous acceleration is change in speed per time.\n",
    "\n",
    "The DeltaPos is calculated using the geodesic function from the geopy package in which the default ellipsoid used to model the earth is the standard WGS-84 ellipsoid.  Geopy documentation can be found here: https://geopy.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered['DeltaPos (mi)']=np.nan\n",
    "sample_trips_filtered.loc[0,'DeltaPos (mi)']=0\n",
    "sample_trips_filtered.loc[0,'Speed MPH']=0\n",
    "sample_trips_filtered.loc[0,'Accel MPHPS']=0\n",
    "sample_trips_filtered.loc[1,'Accel MPHPS']=0\n",
    "sample_trips_filtered['local_dtm']=pd.to_datetime(sample_trips_filtered['local_dtm'],format='%d%b%y:%H:%M:%S')\n",
    "#a['DeltaTime']=pd.to_datetime(a['local_dtm'],infer_datetime_format=True)\n",
    "sample_trips_filtered.loc[0,'DeltaTime (hr)']=0\n",
    "for i in range(1,len(sample_trips_filtered)):\n",
    "    sample_trips_filtered.loc[i,'DeltaPos (mi)']=geodesic((sample_trips_filtered.loc[i-1,'latitude'],sample_trips_filtered.loc[i-1,'longitude']),(sample_trips_filtered.loc[i,'latitude'],sample_trips_filtered.loc[i,'longitude'])).miles \n",
    "    sample_trips_filtered.loc[i,'DeltaTime (hr)']=pd.Timedelta(sample_trips_filtered.loc[i,'local_dtm']-sample_trips_filtered.loc[i-1,'local_dtm']).seconds/3600.\n",
    "    sample_trips_filtered.loc[i,'Speed MPH']=sample_trips_filtered.loc[i,'DeltaPos (mi)']/sample_trips_filtered.loc[i,'DeltaTime (hr)']\n",
    "    if i>1: #need a delta v to calculate instantaneous acceleration otherwise nonsensical information, Not great to check every iteration but sufficient\n",
    "        sample_trips_filtered.loc[i,'Accel MPHPS']=(sample_trips_filtered.loc[i,'Speed MPH']-sample_trips_filtered.loc[i-1,'Speed MPH'])/(sample_trips_filtered.loc[i,'DeltaTime (hr)']*3600.)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,2,figsize=(15,10))\n",
    "sample_trips_filtered['DeltaPos (mi)'].plot(ax=axs[0,0],title=\"DeltaPosition (mi)\")\n",
    "sample_trips_filtered['DeltaTime (hr)'].plot(ax=axs[0,1],title=\"DeltaTime (hr)\")\n",
    "sample_trips_filtered['Speed MPH'].plot(ax=axs[1,0],title=\"Speed (MPH)\")\n",
    "sample_trips_filtered['Accel MPHPS'].plot(ax=axs[1,1],title='Accel MPHPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further issues for cleaning consideration\n",
    "From these plots we can infer 2 issues.  First, from the DeltaPosition and DeltaTime plots there are a large number of drifts.  These drifts account for gps drift between trips. One can count the peaks, in particular in DeltaTime and see there are 15 visible peaks which in addition to the first trip are the 16 total trips accounted for in the dataset.  This can be shown quantitatively by looking at the amount of times the DeltaTime between two points is greater than 36 seconds (0.01hours).  These points should not cause issues in estimations of speed and number of hard acceleration events.  This is implied because they do not show up in the speed and acceleration plots and I can look at trips individually which after further cleaning will be self consistent to themselves.\n",
    "\n",
    "Secondly, it can see  speed and acceleration plots something odd is going on in at least 3 further points by the points that seem unphysical (i.e. hundreds of mph/s accelerations which are then mirrored or speeds well in excess of 600mph.  These points come from errors in reading for a few seconds.  These points need to be dealt while.\n",
    "\n",
    "A third issue comes when the DeltaTime between two events is 0, this could easily occur if the recording device has an error in the frequency at which it records, i.e. if the frequency drops below 1Hz and you take two readings within a second.  To deal with the additional points will be dropped. The alternative is averaging the longitude,latitude values but as any single second will be a small change the average would not have much of an effect that isnt averaged out in the acceleration.  It would be more computationally appropriate to check the timestamps first and average the indices that way however as an exercise I feel it is important to show how I came about this solution and the order in which I decided to clean the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_trips_filtered[sample_trips_filtered['DeltaTime (hr)']>0.01].index,'\\n Number of time intervals greater than 36seconds (0.01hr):', len(sample_trips_filtered[sample_trips_filtered['DeltaTime (hr)']>0.1].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Cleaning\n",
    "Here is a list of the times there is 0 change in time from the previous recorded datapoint.  It seems to happen in smallish clusters for the most part just by looking at the indices.  The first point is obviously ok as it is the initial time period and DeltaTime (hr) was initialized to 0 by myself.  This happens 24 additional times\n",
    "\n",
    "Now that we will filter out the repeat time points and remake the dataframe with the speed/acceleration etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidenceList = sample_trips_filtered[sample_trips_filtered['DeltaTime (hr)']==0.].index\n",
    "print(coincidenceList[1:])\n",
    "sample_trips_filtered2=sample_trips_filtered.drop(sample_trips_filtered.index[coincidenceList[1:]])\n",
    "sample_trips_filtered2 = sample_trips_filtered2.reset_index(drop=True)\n",
    "\n",
    "sample_trips_filtered2['DeltaPos (mi)']=np.nan\n",
    "sample_trips_filtered2.loc[0,'DeltaPos (mi)']=0\n",
    "sample_trips_filtered2.loc[0,'Speed MPH']=0\n",
    "sample_trips_filtered2.loc[0,'Accel MPHPS']=0\n",
    "sample_trips_filtered2.loc[1,'Accel MPHPS']=0\n",
    "#sample_trips_filtered2['local_dtm']=pd.to_datetime(sample_trips_filtered2['local_dtm'],format='%d%b%y:%H:%M:%S')\n",
    "sample_trips_filtered2.loc[0,'DeltaTime (hr)']=0\n",
    "for i in range(1,len(sample_trips_filtered2)):\n",
    "    sample_trips_filtered2.loc[i,'DeltaPos (mi)']=geodesic((sample_trips_filtered2.loc[i-1,'latitude'],sample_trips_filtered2.loc[i-1,'longitude']),(sample_trips_filtered2.loc[i,'latitude'],sample_trips_filtered2.loc[i,'longitude'])).miles \n",
    "    sample_trips_filtered2.loc[i,'DeltaTime (hr)']=pd.Timedelta(sample_trips_filtered2.loc[i,'local_dtm']-sample_trips_filtered2.loc[i-1,'local_dtm']).seconds/3600.\n",
    "    sample_trips_filtered2.loc[i,'Speed MPH']=sample_trips_filtered2.loc[i,'DeltaPos (mi)']/sample_trips_filtered2.loc[i,'DeltaTime (hr)']\n",
    "    if i>1: #need a delta v to calculate instantaneous acceleration otherwise nonsensical information, Not great to check every iteration but sufficient\n",
    "        sample_trips_filtered2.loc[i,'Accel MPHPS']=(sample_trips_filtered2.loc[i,'Speed MPH']-sample_trips_filtered2.loc[i-1,'Speed MPH'])/(sample_trips_filtered2.loc[i,'DeltaTime (hr)']*3600.)\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look around these points in speed, the acceleration curve and mirroring will be taken care of if the speed/position issues are removed.  These are caused by GPS Jumps to a new base point.  Just removing these events would not take care of the issue as the base point has drifted and recalculating the speeds/accelerations after the removal would show the same issues, perhaps even exaggerated more depending on the jumps.\n",
    "One potential solution is to interpolate the speed at the odd indices manually by the speed on either side.  We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample_trips_filtered2[sample_trips_filtered2['Speed MPH']>100.].index))\n",
    "for i in sample_trips_filtered2[sample_trips_filtered2['Speed MPH']>100.].index:\n",
    "    print(sample_trips_filtered2.loc[i-1:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample_trips_filtered2[sample_trips_filtered2['DeltaPos (mi)']>1.].index))\n",
    "#for i in sample_trips_filtered2[sample_trips_filtered2['DeltaPos (mi)']>1.].index:\n",
    "#    print(sample_trips_filtered2.loc[i-1:i+1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered2[(sample_trips_filtered2['Speed MPH']>100.) & (np.abs(sample_trips_filtered2['Accel MPHPS'])>30)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered2[(np.abs(sample_trips_filtered2['Accel MPHPS'])>40)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the points that we want to adapt.  In this dataset they coincide with excessive speed as well as excessive acceleration.  The maximal acceleration by the cars is around 20-25mphps so a large buffer is given to this and the additional speed requirement is required as the acceleration at such a speed would be lower.  Go through, interpolate the speed at those points and recalculate the accleration for the entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeList = sample_trips_filtered2[(sample_trips_filtered2['Speed MPH']>100.) & (np.abs(sample_trips_filtered2['Accel MPHPS'])>30)].index\n",
    "print('Indices to be changed: ',changeList)\n",
    "sample_trips_filtered3=sample_trips_filtered2.copy()\n",
    "for index in changeList:\n",
    "    sample_trips_filtered3.loc[index,'Speed MPH']=(sample_trips_filtered3.loc[index-1,'Speed MPH']+sample_trips_filtered3.loc[index+1,'Speed MPH'])/2\n",
    "for i in range(2,len(sample_trips_filtered3)):\n",
    "    sample_trips_filtered3.loc[i,'Accel MPHPS']=(sample_trips_filtered3.loc[i,'Speed MPH']-sample_trips_filtered3.loc[i-1,'Speed MPH'])/(sample_trips_filtered3.loc[i,'DeltaTime (hr)']*3600.)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,2,figsize=(15,10))\n",
    "sample_trips_filtered3['DeltaPos (mi)'].plot(ax=axs[0,0],title=\"DeltaPosition (mi)\")\n",
    "sample_trips_filtered3['DeltaTime (hr)'].plot(ax=axs[0,1],title=\"DeltaTime (hr)\")\n",
    "sample_trips_filtered3['Speed MPH'].plot(ax=axs[1,0],title=\"Speed (MPH)\")\n",
    "sample_trips_filtered3['Accel MPHPS'].plot(ax=axs[1,1],title='Accel MPHPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results after very basic data cleaning has been applied to remove unphysical events.  For the most part this puts the speed data within both legal possible realms.  Before I start classifying I want to apply a mild rolling average to the speeds and recalculate the accelerations to help remove some of the noise while keeping the general structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered3['SpeedAvg']=sample_trips_filtered3.loc[:,'Speed MPH'].rolling(window=3).mean()\n",
    "sample_trips_filtered3['AccelAvg3']=sample_trips_filtered3.loc[:,'Accel MPHPS'].rolling(window=3).mean()\n",
    "\n",
    "for i in range(2,len(sample_trips_filtered3)):\n",
    "    sample_trips_filtered3.loc[i,'AccelAvg']=(sample_trips_filtered3.loc[i,'SpeedAvg']-sample_trips_filtered3.loc[i-1,'SpeedAvg'])/(sample_trips_filtered3.loc[i,'DeltaTime (hr)']*3600.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(25,10))\n",
    "sample_trips_filtered3['Speed MPH'].plot(title=\"Speed (MPH)\")\n",
    "sample_trips_filtered3['SpeedAvg'].plot(title=\"Speed (MPH)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(25,10))\n",
    "sample_trips_filtered3['Accel MPHPS'].plot(title=\"Accel (MPHPS)\")\n",
    "sample_trips_filtered3['AccelAvg'].plot(title=\"Windowed Accel (MPHPS)\")\n",
    "sample_trips_filtered3['AccelAvg3'].plot(title=\"Windowed Accel (MPHPS)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip by Trip Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading each individual trip into its own object to further examine each trip on an individual basis\n",
    "# \n",
    "trip_dict={}\n",
    "for trip_id in range(1,17):\n",
    "    trip_dict[str(trip_id)]=sample_trips_filtered3[sample_trips_filtered3.trip_nb == trip_id]\n",
    "    #Also need to reset DeltaTime between trips\n",
    "    trip_dict[str(trip_id)].loc[(trip_dict[str(trip_id)].head(1).index[0],'DeltaTime (hr)')]=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(16,2,figsize=(15,25))\n",
    "for i in range(1,17):\n",
    "    axs[0,0].set_title('Speed')\n",
    "    axs[0,1].set_title('Accel')\n",
    "    axs[i-1,0].set_ylabel('Trip '+str(i))\n",
    "    trip_dict[str(i)]['Speed MPH'].plot(ax=axs[i-1,0])\n",
    "    trip_dict[str(i)]['SpeedAvg'].plot(ax=axs[i-1,0])\n",
    "    trip_dict[str(i)]['Accel MPHPS'].plot(ax=axs[i-1,1])\n",
    "    trip_dict[str(i)]['AccelAvg'].plot(ax=axs[i-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting a Threshold for hard events (braking and acceleration) baed on the data.\n",
    "## Hard Braking Events\n",
    "Having no a priori intuition about numbers to use for hard braking and acceleration events I first looked up some baseline numbers\n",
    "Some definitions need to be worked out i.e. what am I considering a hard event.  After some brief cursory research I've found a negative acceleration at around 7mph/s and up to 15mph/s to be standard for 18 wheelers (https://www.michiganautolaw.com/blog/2017/11/05/hard-braking/).  If we use this minimal value as a hard braking cutoff it will be the number of times the acceleration is calculated at <-7mph/s\n",
    "## Hard Acceleration Events\n",
    "For hard accelerations I'm going to set my threshold based on 0-60 times for typical cars, let's take a mid 2000s Corolla for example the 0-60 time is around 8 seconds (https://www.zeroto60times.com/vehicle-make/toyota-0-60-mph-times/).  If we allow for some buffer room above this i.e. around 10 seconds instead of 8 for a hard cutoff on hard accelerations it would be the number of events where the acceleration is above 6mph/s.  I would certainly consider the maximum acceleration for a vehicle in proper working order to be more than a hard acceleration event. However, I recognize that this is very depedent upon the vehicle i.e., a loaded down smart car is most likely incapable of a hard acceleration which by itself could lead to unsafety as hard accelerations can be used to avoid incidents.  Using the averaged value in a window of 3 seconds should be sufficient without smearing out the largest acceleration and braking events.\n",
    "## Idle Time\n",
    "Time spent below while moving with an average (window=3) speed less than 1mph, allows moderate drift while stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets compare how these reference hard acceleration numbers compare to the average acceleration and average braking  values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips_filtered3['AccelAvg3'].plot.hist(bins=20)\n",
    "print(sample_trips_filtered3['AccelAvg3'].mean())\n",
    "print(sample_trips_filtered3['AccelAvg3'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Positive Acceleration: \",sample_trips_filtered3[sample_trips_filtered3['AccelAvg3']>0.]['AccelAvg3'].mean())\n",
    "print(\"Standard Deviation: \",sample_trips_filtered3[sample_trips_filtered3['AccelAvg3']>0.]['AccelAvg3'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Negative Acceleration: \",sample_trips_filtered3[sample_trips_filtered3['AccelAvg3']<0.]['AccelAvg3'].mean())\n",
    "print(\"Standard Deviation: \",sample_trips_filtered3[sample_trips_filtered3['AccelAvg3']<0.]['AccelAvg3'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the average acceleration (using the window=3 rolilng average) is 1.71mph/s compared to the hard acceleration estimation I found to be around 6-7.5 mph/s.  The average braking acceleration (-1.62mph/s) compared to the -7mph/s.  For simplicity going forward I will use assume that the distribution of accelerations is normal enough that I can assume a half normal distribution and use the mean+2standard deviations for acceleration threshold and mean-2standard deviations for the braking acceleration threshold.\n",
    "\n",
    "## Thresholds - Acceleration: 6.47mph/s, Braking: -5.58mph/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a few helper functions to return the amount of hard acceleration events, hard braking events, total idle time, and total distance traveled.  For hard braking and acceleration I am choosing to use the rolling average value and count peaks above my above determined threshold to get individual events and not just the time within those events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHardEvents(df,threshold,AccelBrake):\n",
    "    sign = 1\n",
    "    if AccelBrake == \"Accel\":\n",
    "        sign =1\n",
    "    elif AccelBrake == \"Brake\":\n",
    "        sign =-1\n",
    "    peaks=0\n",
    "    for i in range(df.head(1).index[0]+1,df.tail(1).index[0]-1):\n",
    "        if sign*df.AccelAvg3[i]>sign*df.AccelAvg3[i-1]:\n",
    "            if sign*df.AccelAvg3[i]>sign*df.AccelAvg3[i+1]:\n",
    "                if sign*df.AccelAvg3[i]>threshold:\n",
    "                    #print(i)\n",
    "                    peaks+=1\n",
    "    return peaks\n",
    "\n",
    "def GetIdleTotalTime(df):\n",
    "    IdleTime, TotalTime=0.,0.\n",
    "    try: #handles exception in case of 0 idle time as defined below\n",
    "        IdleTime = df['DeltaTime (hr)'][(df['SpeedAvg'])<1.].cumsum().iloc[-1]\n",
    "    except IndexError:\n",
    "        print(\"No Idle Time for this Trip\")\n",
    "    TotalTime = df['DeltaTime (hr)'].cumsum().iloc[-1]\n",
    "    return IdleTime*60, TotalTime*60\n",
    "\n",
    "def GetIntegralDistance(df):\n",
    "    distance=0\n",
    "    distance= df['DeltaPos (mi)'].cumsum()\n",
    "    return distance.iloc[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,17):\n",
    "    print(\"Trip: \", str(i))\n",
    "    print('\\t Hard Accel Events: %i '%(GetHardEvents(trip_dict[str(i)],6.47,\"Accel\")))\n",
    "    print('\\t Hard Brake Events: %i '%(GetHardEvents(trip_dict[str(i)],5.58,\"Brake\")))\n",
    "    print('\\t Idle Time: %.2f min, \\t Total Time: %.2f min'%(GetIdleTotalTime(trip_dict[str(i)])))\n",
    "    print('\\t Distance Traveled: %.2f mi \\n'%(GetIntegralDistance(trip_dict[str(i)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot = pd.read_csv(r\"C:\\Users\\JTBar\\Documents\\Telematics Exercise Files\\simulated_summary_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot.groupby(['VehicleType'])['Loss'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This dataset contains 30,000 vehicles, of which 4031 have been in a collision.  For a further breakdown of how many vehicles have been in a collision by VehicleType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot.groupby(['VehicleType','Loss'])['Loss'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Is there a statistically significant difference between vehicle types?\n",
    "If we assume the loss populations are sampled from a binomial distribution with probability given by TotalLossPerType/TotalType a simple z-test can be conducted to determine if the null hypothesis, the distributions being 'sampled' from a similar distribution, can be rejected.  For a significance $\\alpha = 0.05$ a z-value greater than the critical value of 1.64 means the null hypothesis is rejected and the population proportions are statistically significantly different at the 0.05 significance level.  However, there is a nonzero chance that continually looking at distributions will result in a positive effect (The Look-Elsewhere effect) one way to combat this is to divide the significance value youre looking for by the number of unique trials (here 6) and using that critical value.  This then is $\\alpha = 0.083$ and chances $z_{critical}=2.64$.  However, the choice of p<0.05 rejecting the null hypothesis is a convention and the distinction here is somewhat arbitrary.  The conclusions that I draw depend on how liberal we want to be in the definition of statistical significance.\n",
    "\\begin{equation*}\n",
    "z   = \\frac{p_1 - p_2}{\\sqrt{p (1-p)(1/n_1 + 1/n_2)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProbDict={}\n",
    "ProbDict['Car']     = [1130.,9085.];\n",
    "ProbDict['Minivan'] = [155.,1520.] ;\n",
    "ProbDict['SUV']     = [1095.,7463.];\n",
    "ProbDict['Truck']   = [1651.,11932.];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateZ(ProbDict,Vehicle1,Vehicle2):\n",
    "    #Takes the total sample size of two distributions and number of 'favorable' cases here the Loss and returns a z-test value\n",
    "    n1=ProbDict[Vehicle1][1]\n",
    "    x1=ProbDict[Vehicle1][0]\n",
    "    n2=ProbDict[Vehicle2][1]\n",
    "    x2=ProbDict[Vehicle2][0]\n",
    "    p1 = x1/n1\n",
    "    p2 = x2/n2\n",
    "    p  = (x1+x2)/(n1+n2)\n",
    "    z  = np.abs(p1-p2)/np.sqrt(p*(1-p)*(1/n1+1/n2))\n",
    "    print('z value for %s and %s: %.2f'%(Vehicle1,Vehicle2,z))\n",
    "    return z\n",
    "usedKeys=[]\n",
    "for key in ProbDict:\n",
    "    usedKeys.append(key)\n",
    "    for key2 in ProbDict:\n",
    "        if key !=key2 and key2 not in usedKeys:\n",
    "            calculateZ(ProbDict,key,key2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying this z test for all possible combinations all combinations can be said to be statistically significant except for the combination of populations of Trucks and SUVs.  Including the Look-elsewhere effect also can bring back into statistical insignificance the combination of Cars/Minivans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Are hard brakes and hard accelerations equally important in predicting risks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hard Brakes per Loss Event- Mean: %.2f, Stdev: %.2f, Median: %.2f \"%(sim_sum_tot[sim_sum_tot['Loss']==1]['HardBrakes'].mean(),sim_sum_tot[sim_sum_tot['Loss']==1]['HardBrakes'].std(), sim_sum_tot[sim_sum_tot['Loss']==1]['HardBrakes'].median()))\n",
    "print(\"Hard Accelerations per Loss Event- Mean: %.2f, Stdev: %.2f, Median: %.2f \"%(sim_sum_tot[sim_sum_tot['Loss']==1]['HardAccelerations'].mean(),sim_sum_tot[sim_sum_tot['Loss']==1]['HardAccelerations'].std(),sim_sum_tot[sim_sum_tot['Loss']==1]['HardAccelerations'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hard Brakes per 0 Loss Event- Mean: %.2f, Stdev: %.2f, Median: %.2f \"%(sim_sum_tot[sim_sum_tot['Loss']==0]['HardBrakes'].mean(),sim_sum_tot[sim_sum_tot['Loss']==0]['HardBrakes'].std(), sim_sum_tot[sim_sum_tot['Loss']==0]['HardBrakes'].median()))\n",
    "print(\"Hard Accelerations per 0 Loss Event- Mean: %.2f, Stdev: %.2f, Median: %.2f \"%(sim_sum_tot[sim_sum_tot['Loss']==0]['HardAccelerations'].mean(),sim_sum_tot[sim_sum_tot['Loss']==0]['HardAccelerations'].std(),sim_sum_tot[sim_sum_tot['Loss']==0]['HardAccelerations'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot[sim_sum_tot['Loss']==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_sum_tot[sim_sum_tot['Loss']==0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "This model will be employeeing a densely connected feed forward neural network for event classification\n",
    "Activation functions: \n",
    "    rectified linear unit (ReLU) activation function such that the gradients of activation functions will not diminish as you increase the depth of the network\n",
    "    sigmoid activation function is applied to the output layer as is standard for classification problems\n",
    "    \n",
    "Loss/Response Function:\n",
    "    Binary Cross Entropy will be used to calculate the predicted probability (probability of correct classification based on the input values)  Larger predicted probabilites correspond to lower response function values for correctly identified events.\n",
    "\n",
    "Optimization Function:\n",
    "    Adam (Adaptive moment estimation) Optimizer which is the standard efficient optimization function that calulates adaptive learning rates for all parameters in the network.  It computes exponentially weighted averages of past gradients and squares of gradients\n",
    "    \n",
    "I'll split up the input dataset into 2 dataframes, X:the useful input values (dropping Loss as it is the classifier, Vehicle which is effectively an index, and Days which is 365 for every row, and y: the truth value of loss prediction\n",
    "\n",
    "First I'll show an extremely naive approach just taking in the disparate raw datasets before moving onto using the same approach with oversampled minority case events using both the SMOTE and ADASYN oversampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum_tot = sim_sum_tot.replace(\"Car\", 0)\n",
    "sim_sum_tot = sim_sum_tot.replace(\"SUV\", 1)\n",
    "sim_sum_tot = sim_sum_tot.replace(\"Truck\", 2)\n",
    "sim_sum_tot = sim_sum_tot.replace(\"Minivan\", 3)\n",
    "\n",
    "X = sim_sum_tot.drop(['Loss','Vehicle', 'Days'], axis=1)\n",
    "y = sim_sum_tot['Loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up the signal and background datasets into training sets, testing sets, and validation sets. \n",
    "#This splits into 64% to be used for training and 16% testing and 16% validation\n",
    "ix = range(y.shape[0]) \n",
    "X_train, X_test, y_train, y_test, ix_train, ix_test = train_test_split(X, y, ix, train_size=0.8)\n",
    "X_train, X_val,y_train, y_val,ix_train, ix_val=train_test_split(X_train,y_train,ix_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural networks prefer inputs that are similar to each other while at the same time as normally distributed as possible\n",
    "#sklearn has a variety of scalers that can be used to perform this transformation, here I use RobustScalar\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Function for making various depths and complexities of dense neural networks\n",
    "def DNNmodel(Input_shape=(10,), n_hidden=1, n_nodesHidden=20, dropout=0.2, optimizer='adam'):\n",
    "        inputs=Input(shape=Input_shape)\n",
    "        i=0\n",
    "        if n_hidden>0:\n",
    "                hidden=Dense(n_nodesHidden, activation='relu')(inputs)\n",
    "                hidden=Dropout(dropout)(hidden)\n",
    "                i+=1\n",
    "        while i<n_hidden:\n",
    "                hidden=Dense(n_nodesHidden, activation='relu')(hidden)\n",
    "                hidden=Dropout(dropout)(hidden)\n",
    "                i+=1\n",
    "        outputs = Dense(1,activation='sigmoid')(hidden)\n",
    "        model = Model(inputs,outputs)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNmodel(Input_shape=(5,),n_hidden=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=50,callbacks=[EarlyStopping(verbose=True,patience=10,monitor='val_loss')], validation_data=(X_val, y_val),batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history\n",
    "print(\"history keys: \", history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy plot\n",
    "plt.plot(100 * np.array(history['accuracy']), label='training')\n",
    "plt.plot(100 * np.array(history['val_accuracy']), label='validation')\n",
    "plt.xlim(0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy %')\n",
    "plt.legend(loc='lower right', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test, verbose = True, batch_size = 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cls = np.argmax(yhat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 20)\n",
    "_ = plt.hist(yhat[y_test==1], histtype='stepfilled', alpha=0.5, color='red', label=r\"Loss\", bins=bins)\n",
    "_ = plt.hist(yhat[y_test==0], histtype='stepfilled', alpha=0.5, color='blue', label=r'NoLoss', bins=bins)\n",
    "plt.axvline(x=0.5, linestyle='--')\n",
    "plt.legend(loc='upper right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAcc = yhat[y_test==1].round().sum()/y_test.sum()\n",
    "print(\"Loss Event Accuracy: %.1f%%\" %(LossAcc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious from this that the neural network, at least with the sample given is not sufficient without taking into consideration the disparate nature of the signal/background like events. Even though there is clearly a shape difference in the distributions all events are classified as 'No Loss' because of the large pull from the significant number of 'No Loss' events compared to 'Loss' Events.  The network accuracy goes to the percentage of events in the majority case while classifying all signal events incorrectly. To account for this I will preform oversampling\n",
    "\n",
    "# Resampling, NN part 2\n",
    "Let's try to do some resampling using Adaptive Synthetic Sampling, focuses on data that are hard to learn, fills in more edge cases than the next technique (SMOTE) which fills more linearlly between events.\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upscale the minority class such that each class has roughly equal representation\n",
    "from imblearn.over_sampling import ADASYN\n",
    "adasyn = ADASYN(sampling_strategy=\"auto\")\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "np.bincount(y_train_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNmodel(Input_shape=(5,),n_hidden=3);\n",
    "model.fit(X_train_adasyn,y_train_adasyn,epochs=100,callbacks=[EarlyStopping(verbose=True,patience=20,monitor='val_loss')], validation_data=(X_val, y_val),batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history\n",
    "print(\"history keys: \", history.keys())\n",
    "#Accuracy plot\n",
    "plt.plot(100 * np.array(history['accuracy']), label='training')\n",
    "plt.plot(100 * np.array(history['val_accuracy']), label='validation')\n",
    "plt.xlim(0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy %')\n",
    "plt.legend(loc='lower right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test, verbose = True, batch_size = 512) \n",
    "yhat_cls = np.argmax(yhat, axis=1)\n",
    "sum(yhat_cls)\n",
    "bins = np.linspace(0, 1, 20)\n",
    "_ = plt.hist(yhat[y_test==1], histtype='stepfilled', alpha=0.5, color='red', label=r\"Signal\", bins=bins)\n",
    "_ = plt.hist(yhat[y_test==0], histtype='stepfilled', alpha=0.5, color='blue', label=r'Background', bins=bins)\n",
    "plt.axvline(x=0.5,linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss Events: mean: %.3f, std: %.3f\" %(yhat[y_test==1].mean(),yhat[y_test==1].std()))\n",
    "print(\"NoLoss Events: mean: %.3f, std: %.3f\" %(yhat[y_test==0].mean(),yhat[y_test==0].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAcc = yhat[y_test==1].round().sum()/y_test.sum()\n",
    "print(\"Loss Event Accuracy: %.1f%%\" %(LossAcc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling using SMOTE\n",
    "Synthetic minority oversampling technique (Smote) generates synthetic data that is similar to, but not exactly like the minority class using nearest-neightbors approach to fill in space between neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\")\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "np.bincount(y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNmodel(Input_shape=(5,),n_hidden=3);\n",
    "model.fit(X_train_smote,y_train_smote,epochs=100,callbacks=[EarlyStopping(verbose=True,patience=20,monitor='val_loss')], validation_data=(X_val, y_val),batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history\n",
    "print(\"history keys: \", history.keys())\n",
    "#Accuracy plot\n",
    "plt.plot(100 * np.array(history['accuracy']), label='training')\n",
    "plt.plot(100 * np.array(history['val_accuracy']), label='validation')\n",
    "plt.xlim(0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy %')\n",
    "plt.legend(loc='lower right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test, verbose = True, batch_size = 512) \n",
    "bins = np.linspace(0, 1, 20)\n",
    "_ = plt.hist(yhat[y_test==1], histtype='stepfilled', alpha=0.5, color='red', label=r\"Signal\", bins=bins)\n",
    "_ = plt.hist(yhat[y_test==0], histtype='stepfilled', alpha=0.5, color='blue', label=r'Background', bins=bins)\n",
    "plt.axvline(x=0.5,linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss Events: mean: %.3f, std: %.3f\" %(yhat[y_test==1].mean(),yhat[y_test==1].std()))\n",
    "print(\"NoLoss Events: mean: %.3f, std: %.3f\" %(yhat[y_test==0].mean(),yhat[y_test==0].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAcc = yhat[y_test==1].round().sum()/y_test.sum()\n",
    "print(\"Loss Event Accuracy: %.1f%%\" %(LossAcc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a BDT instead of a NN, with ADASYN Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to XGBoost friendlier format, using all potentially useful columns, dropping Vehicle (an index) and Days (always 365, no chance of separation between loss type events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up the signal and background datasets into training sets, testing sets, and validation sets. \n",
    "#This splits into 80% to be used for training and 20% testing\n",
    "X = sim_sum_tot.drop(['Vehicle', 'Days'], axis=1)\n",
    "y = sim_sum_tot['Loss']\n",
    "ix = range(y.shape[0]) \n",
    "#X=X.drop('Loss',axis=1)\n",
    "X_train, X_test, y_train, y_test, ix_train, ix_test = train_test_split(X, y, ix, train_size=0.8)\n",
    "#X_train, X_val,y_train, y_val,ix_train, ix_val=train_test_split(X_train,y_train,ix_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = ADASYN(sampling_strategy=\"auto\")\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "np.bincount(y_train_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training samples: {}'.format(len(X_train)))\n",
    "print('Number of testing samples: {}'.format(len(X_test)))\n",
    "print('ADASYN: Number of training samples: {}'.format(len(X_train_adasyn)))\n",
    "\n",
    "print('\\nNumber of signal events in training set: {}'.format(len(X_train[X_train.Loss == 1])))\n",
    "print('ADASYN:Number of signal events in training set: {}'.format(len(X_train_adasyn[X_train_adasyn.Loss == 1])))\n",
    "print('Number of background events in training set: {}'.format(len(X_train[X_train.Loss == 0])))\n",
    "print('Fraction signal: {}'.format(len(X_train[X_train.Loss == 1])/(float)(len(X_train[X_train.Loss == 1]) + len(X_train[X_train.Loss == 0]))))\n",
    "print('ADASYN: Fraction signal: {}'.format(len(X_train_adasyn[X_train_adasyn.Loss == 1])/(float)(len(X_train_adasyn[X_train_adasyn.Loss == 1]) + len(X_train_adasyn[X_train_adasyn.Loss == 0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns[:-1]  # we skip the last column because it is the Loss label\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_bdt_param = {\n",
    "    \"learning_rate\" : 0.15,\n",
    "    \"max_depth\" : 6,\n",
    "    \"colsample_bytree\" : 1.0,\n",
    "    \"subsample\" : 1.0,\n",
    "    \"n_estimators\" : 200,\n",
    "    \"feature_names\" : features,\n",
    "    'objective' : 'binary:logistic' # objective function\n",
    "}\n",
    "binary_task_param = {\n",
    "    \"eval_metric\" : [\"logloss\",\"error\"],\n",
    "    \"early_stopping_rounds\" : 30,\n",
    "    \"eval_set\": [(X_train_adasyn[features],y_train_adasyn), \n",
    "                 (X_test[features],y_test)]\n",
    "}\n",
    "\n",
    "binary_bdt = xgb.XGBClassifier(**binary_bdt_param)\n",
    "binary_bdt.fit(X_train_adasyn[features], y_train_adasyn,\n",
    "              verbose=False, **binary_task_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_df = X_test.copy()\n",
    "evaluated_df[\"binary_prob\"] = binary_bdt.predict_proba(X_test[features])[:,1]\n",
    "print(binary_bdt.score(X_test[features],y_test))\n",
    "\n",
    "binary_bdt.predict_proba(X_test[features])[:,1].round().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_bdt.predict(X_test[features])\n",
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_bdt.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "predictions = binary_bdt.predict(X_test[features])\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "thresholds = np.sort(binary_bdt.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(binary_bdt, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train_adasyn[features])\n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier()\n",
    "    selection_model.fit(select_X_train, y_train_adasyn)\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test[features])\n",
    "    predictions = selection_model.predict(select_X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Loss Event Accuracy: %.2f%%\"%(predictions.sum()/y_test.sum()*100.))\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_enum = plt.subplots(1,2, figsize=(16,4))\n",
    "xgb.plot_importance(binary_bdt, importance_type=\"weight\", ax=ax_enum[0], title=\"Weight\",show_values=False, grid=False)\n",
    "xgb.plot_importance(binary_bdt, importance_type=\"gain\", ax=ax_enum[1], title=\"Gain\", show_values=False, grid=False)\n",
    "plt.ylabel(\"\")\n",
    "plt.sca(ax_enum[1])\n",
    "plt.ylabel(\"\")\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importance_type=Gain is how useful is the variable in terms of separation where importance_type=Weight is how frequently splitting on the variable occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all predictions (both signal and background)\n",
    "predictions = binary_bdt.predict_proba(X_test[features])[:,1]\n",
    "plt.figure();\n",
    "plt.hist(predictions,bins=np.linspace(0,1,50),histtype='step',color='darkgreen',label='All events');\n",
    "# make the plot readable\n",
    "plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "plt.ylabel('Events',fontsize=12);\n",
    "plt.legend(frameon=False);\n",
    "\n",
    "# plot signal and background separately\n",
    "plt.figure();\n",
    "plt.hist(binary_bdt.predict_proba(X_test[features][y_test==1])[:,1],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='midnightblue',label='Loss');\n",
    "plt.hist(binary_bdt.predict_proba(X_test[features][y_test==0])[:,1],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='firebrick',label='No Loss');\n",
    "# make the plot readable\n",
    "plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "plt.ylabel('Events',fontsize=12);\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure();\n",
    "plt.hist(X_train.Distance[X_train.Loss == 1],bins=np.linspace(0,40000,50),\n",
    "         histtype='step',color='midnightblue',label='Loss');\n",
    "plt.hist(X_train.Distance[X_train.Loss == 0],bins=np.linspace(0,40000,50),\n",
    "         histtype='step',color='firebrick',label='No Loss');\n",
    "\n",
    "plt.xlabel('Distance',fontsize=12);\n",
    "plt.ylabel('Events',fontsize=12);\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "#plt.plot(X_train_adasyn.Distance[X_train_adasyn.Loss == 0],X_train_adasyn.NightTime_Pct[X_train_adasyn.Loss == 0],\n",
    "#         'o',markersize=2,color='firebrick',markeredgewidth=0,alpha=0.8,label='NoLoss');\n",
    "#plt.plot(X_train_adasyn.Distance[X_train_adasyn.Loss == 1],X_train_adasyn.NightTime_Pct[X_train_adasyn.Loss == 1],\n",
    "#         'o',markersize=2,color='mediumblue',markeredgewidth=0,alpha=0.8,label='Loss');\n",
    "\n",
    "plt.plot(X_train.Distance[X_train.Loss == 0],X_train.NightTime_Pct[X_train.Loss == 0],\n",
    "         'o',markersize=2,color='firebrick',markeredgewidth=0,alpha=0.8,label='NoLoss');\n",
    "plt.plot(X_train.Distance[X_train.Loss == 1],X_train.NightTime_Pct[X_train.Loss == 1],\n",
    "         'o',markersize=2,color='mediumblue',markeredgewidth=0,alpha=0.8,label='Loss');\n",
    "\n",
    "plt.xlim(0,35000);\n",
    "plt.ylim(0,1);\n",
    "plt.xlabel('Distance',fontsize=12);\n",
    "plt.ylabel('NightTime_Pct',fontsize=12);\n",
    "plt.legend(frameon=False,numpoints=1,markerscale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDT with SMOTE resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to XGBoost friendlier format, using all potentially useful columns, dropping Vehicle (an index) and Days (always 365, no chance of separation between loss type events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up the signal and background datasets into training sets, testing sets, and validation sets. \n",
    "#This splits into 80% to be used for training and 20% testing\n",
    "X = sim_sum_tot.drop(['Vehicle', 'Days'], axis=1)\n",
    "y = sim_sum_tot['Loss']\n",
    "ix = range(y.shape[0]) \n",
    "#X=X.drop('Loss',axis=1)\n",
    "X_train, X_test, y_train, y_test, ix_train, ix_test = train_test_split(X, y, ix, train_size=0.8)\n",
    "#X_train, X_val,y_train, y_val,ix_train, ix_val=train_test_split(X_train,y_train,ix_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = SMOTE(sampling_strategy=\"auto\")\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "np.bincount(y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training samples: {}'.format(len(X_train)))\n",
    "print('Number of testing samples: {}'.format(len(X_test)))\n",
    "print('SMOTE: Number of training samples: {}'.format(len(X_train_smote)))\n",
    "\n",
    "print('\\nNumber of signal events in training set: {}'.format(len(X_train[X_train.Loss == 1])))\n",
    "print('SMOTE:Number of signal events in training set: {}'.format(len(X_train_smote[X_train_smote.Loss == 1])))\n",
    "print('Number of background events in training set: {}'.format(len(X_train[X_train.Loss == 0])))\n",
    "print('Fraction signal: {}'.format(len(X_train[X_train.Loss == 1])/(float)(len(X_train[X_train.Loss == 1]) + len(X_train[X_train.Loss == 0]))))\n",
    "print('SMOTE: Fraction signal: {}'.format(len(X_train_smote[X_train_smote.Loss == 1])/(float)(len(X_train_adasyn[X_train_smote.Loss == 1]) + len(X_train_smote[X_train_smote.Loss == 0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns[:-1]  # we skip the last column because it is the Loss label\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_bdt_param = {\n",
    "    \"learning_rate\" : 0.15,\n",
    "    \"max_depth\" : 6,\n",
    "    \"colsample_bytree\" : 1.0,\n",
    "    \"subsample\" : 1.0,\n",
    "    \"n_estimators\" : 200,\n",
    "    \"feature_names\" : features,\n",
    "    'objective' : 'binary:logistic' # objective function\n",
    "}\n",
    "binary_task_param = {\n",
    "    \"eval_metric\" : [\"logloss\",\"error\"],\n",
    "    \"early_stopping_rounds\" : 30,\n",
    "    \"eval_set\": [(X_train_adasyn[features],y_train_adasyn), \n",
    "                 (X_test[features],y_test)]\n",
    "}\n",
    "\n",
    "binary_bdt = xgb.XGBClassifier(**binary_bdt_param)\n",
    "binary_bdt.fit(X_train_smote[features], y_train_smote,\n",
    "              verbose=False, **binary_task_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_df = X_test.copy()\n",
    "evaluated_df[\"binary_prob\"] = binary_bdt.predict_proba(X_test[features])[:,1]\n",
    "print(binary_bdt.score(X_test[features],y_test))\n",
    "\n",
    "binary_bdt.predict_proba(X_test[features])[:,1].round().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_bdt.predict(X_test[features])\n",
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_bdt.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "predictions = binary_bdt.predict(X_test[features])\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "thresholds = np.sort(binary_bdt.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(binary_bdt, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train_smote[features])\n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier()\n",
    "    selection_model.fit(select_X_train, y_train_smote)\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test[features])\n",
    "    predictions = selection_model.predict(select_X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Loss Event Accuracy: %.2f%%\"%(predictions.sum()/y_test.sum()*100.))\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_enum = plt.subplots(1,2, figsize=(16,4))\n",
    "xgb.plot_importance(binary_bdt, importance_type=\"weight\", ax=ax_enum[0], title=\"Weight\",show_values=False, grid=False)\n",
    "xgb.plot_importance(binary_bdt, importance_type=\"gain\", ax=ax_enum[1], title=\"Gain\", show_values=False, grid=False)\n",
    "plt.ylabel(\"\")\n",
    "plt.sca(ax_enum[1])\n",
    "plt.ylabel(\"\")\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importance_type=Gain is how useful is the variable in terms of separation where importance_type=Weight is how frequently splitting on the variable occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all predictions (both signal and background)\n",
    "predictions = binary_bdt.predict_proba(X_test[features])[:,1]\n",
    "plt.figure();\n",
    "plt.hist(predictions,bins=np.linspace(0,1,50),histtype='step',color='darkgreen',label='All events');\n",
    "# make the plot readable\n",
    "plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "plt.ylabel('Events',fontsize=12);\n",
    "plt.legend(frameon=False);\n",
    "\n",
    "# plot signal and background separately\n",
    "plt.figure();\n",
    "plt.hist(binary_bdt.predict_proba(X_test[features][y_test==1])[:,1],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='midnightblue',label='Loss');\n",
    "plt.hist(binary_bdt.predict_proba(X_test[features][y_test==0])[:,1],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='firebrick',label='No Loss');\n",
    "# make the plot readable\n",
    "plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "plt.ylabel('Events',fontsize=12);\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
